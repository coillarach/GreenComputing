{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4995d58a",
   "metadata": {},
   "source": [
    "# Topic modelling\n",
    "\n",
    "This script uses a topic modelling technique called [latent Dirichlet allocation](https://dl.acm.org/doi/10.5555/944919.944937) (LDA) to categorise a corpus of 1643 publication abstracts downloaded from the Web of Science (WoS). The items were extracted using the search terms *green computing* OR *green IT* OR *sustainable computing* OR *sustainable IT*. Some manual filtering was performed to identify and remove papers where the match was coincidental (for example, where the abstract contained the string \"... green. It ...\"). \n",
    "\n",
    "The dataset is divided into fields labelled by [WoS tags](https://support.clarivate.com/ScientificandAcademicResearch/s/article/Web-of-Science-Core-Collection-List-of-field-tags-in-output?language=en_US) which include the title, abstract and keywords. In an additional step a *review* attribute has been added which identifies review and survey items. These are eliminated from the default analysis below. However, the code can be altered to experiment with different sets of items.\n",
    "\n",
    "The analysis itself is based on the example explained on the site [Towards Data Science](https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db15ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "import en_core_web_md\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaMulticore\n",
    "from gensim.models import CoherenceModel\n",
    "from IPython.display import HTML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa4919",
   "metadata": {},
   "source": [
    "### Step 1: prepare raw data\n",
    "\n",
    "After importing all the required packages, the first step is to load the WoS dataset from an offline JSON file. Items with no abstract are eliminated, and review articles are filtered out. If you want to try the same analysis on the review articles on their own or to include the review articles with the others, just modify the third line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ccbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_df = pd.read_json('../shared/wos.json')\n",
    "wos_df = wos_df[~wos_df['AB'].isna()]\n",
    "wos_df = wos_df[~wos_df['review']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939ad405",
   "metadata": {},
   "source": [
    "### Step 2: Preprocess\n",
    "\n",
    "Natural language processing is performed using a pre-trained model from [spaCy](https://spacy.io/). This includes\n",
    "\n",
    "* tokenization\n",
    "* lemmatization\n",
    "* stopword removal\n",
    "\n",
    "This takes a long time and a lot of memory and so it has been done in advance. The default code below loads the results from a [pickle](https://docs.python.org/3.9/library/pickle.html) file. The commented code performs the complete process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140d6362",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_df = pd.read_pickle('../shared/wos.pkl')\n",
    "\n",
    "# nlp = en_core_web_md.load()\n",
    "# removal = ['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE', 'NUM', 'SYM']\n",
    "\n",
    "# tokens = []\n",
    "# for summary in nlp.pipe(wos_df['AB']):\n",
    "#    proj_tok = [token.lemma_.lower() for token in summary if token.pos_ not in removal and not token.is_stop and token.is_alpha]\n",
    "#    tokens.append(proj_tok)\n",
    "    \n",
    "# wos_df['tokens'] = tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ce9b31",
   "metadata": {},
   "source": [
    "### Step 3: Create dictionary and corpus\n",
    "\n",
    "Dictionary: list of tokens with unique IDs\n",
    "\n",
    "Corpus: List of pairs made up of token ID and frequency (i.e. term frequency distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50979c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary(wos_df['tokens'])\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=1000)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in wos_df['tokens']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91578551",
   "metadata": {},
   "source": [
    "### Step 4: Build model\n",
    "\n",
    "In an LDA analysis, the number of topics is set in advance. For a given number of topics, a *coherence score* can be calculated which is an indicator of the semantic similarity between the high-scoring words that characterise each topic. To find the optimum number of topics - i.e. the number of topics that optimises the coherence score - the LDA has to be run several times for a range of topic number values. Because this exercise takes a lot of resources and time to run, it has already been done for this corpus. The default code below recovers a stored set of data in order to produce the chart. \n",
    "\n",
    "The commented code goes through the whole process, running the LDA 20 times. This should **not** be done if you are following this tutorial as part of a class to avoid reducing performance for other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = pd.read_csv('../shared/topics.csv')\n",
    "topic_df['new_index'] = [i+1 for i in topic_df.index]\n",
    "topic_df.set_index('new_index', inplace=True)\n",
    "ax=topic_df.plot(figsize=(10, 4))\n",
    "ax.get_legend().remove()\n",
    "ax.set_xlabel(\"Number of Topics\")\n",
    "ax.set_xticks(range(0,20))\n",
    "ax.set_ylabel(\"Coherence Score\")\n",
    "\n",
    "# topics = []\n",
    "# score = []\n",
    "# for i in range(1,20,1):\n",
    "#    lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, random_state=42, iterations=10, num_topics=i, workers = 4, passes=10)\n",
    "#    cm = CoherenceModel(model=lda_model, texts = wos_df['tokens'], corpus=corpus, dictionary=dictionary, coherence='c_v')\n",
    "#    topics.append(i)\n",
    "#    score.append(cm.get_coherence())\n",
    "#    print(i)\n",
    "# _=plt.figure(figsize=(10, 4))\n",
    "# _=plt.plot(topics, score)\n",
    "# _=plt.xticks(range(0,20))\n",
    "# _=plt.xlabel('Number of Topics')\n",
    "# _=plt.ylabel('Coherence Score')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d27f9b2",
   "metadata": {},
   "source": [
    "Once the optimum number of topics has been found, the LDA for that number can be run again so that the results can be examined. According to the chart produced in the previous step, there are two candidates for the number of topics. The default code below uses the value 14 which on the one hand will result in a richer set of topics. On the other hand, some of the topics may be closely related from a semantic point of view. Using the lower value of 6 topics, each topic will be broader but is likely to be more clearly differentiated from the others.\n",
    "\n",
    "Greater detail and a clearer distinction between topics are both useful characteristics. Different situations call for different decisions and sometimes the best way to select the best approach for your context is to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5eefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_topics = 14\n",
    "lda_model = LdaMulticore(corpus=corpus, id2word=dictionary, random_state=42, iterations=50, num_topics=number_of_topics, workers = 4, passes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cbe606",
   "metadata": {},
   "source": [
    "### Step 5: Visualise topics\n",
    "\n",
    "The Python package [pyLDAVis](https://pyldavis.readthedocs.io/en/latest/readme.html) provides a convenient way to explore the results of an LDA analysis. In the interactive diagram that is created, you can select one topic at a time and see the set of terms ordered by their significance. For the details of how this is worked out, please refer to the pyLDAVis documentation.\n",
    "\n",
    "A feature of the interactive visualisation allows you to set the *relevance* parameter &lambda;. Again, a certain amount of experimentation is needed, but in general a value of &lambda; between around 0.6 and 1.0 gives the most accurate results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ffd249",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972520e2",
   "metadata": {},
   "source": [
    "# Exercise 1: name the topics\n",
    "\n",
    "The topics in the visualisation are identified using statistics and have placeholders *Topic 1*, *Topic 2*, etc.  as names. It is up to you as a human being to interpret the frequently used terms in each topic to decide what they represent. Try to aim for topic names that are noun phrases of no more than five words.\n",
    "\n",
    "Double-click the next cell to edit it and enter your topic names. Click *Run* in the toolbar when you are finished."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db91016",
   "metadata": {},
   "source": [
    "* **Topic 1**: your topic name here\n",
    "* **Topic 2**:\n",
    "* **Topic 3**:\n",
    "* **Topic 4**:\n",
    "* **Topic 5**:\n",
    "* **Topic 6**:\n",
    "* **Topic 7**:\n",
    "* **Topic 8**:\n",
    "* **Topic 9**:\n",
    "* **Topic 10**:\n",
    "* **Topic 11**:\n",
    "* **Topic 12**:\n",
    "* **Topic 13**:\n",
    "* **Topic 14**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4ae332",
   "metadata": {},
   "source": [
    "### Show documents by topic\n",
    "\n",
    "To examine the papers in each topic, we first need to associate each document with its primary topic number. Run the code below to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93713ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wos_df['topic'] = [sorted(lda_model[corpus][text])[0][0] for text in range(len(wos_df['AB']))]\n",
    "wos_df.topic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7d99e9",
   "metadata": {},
   "source": [
    "However, note that the pyLDAVis visualisation numbers and orders the topic by the percentage coverage of the top 30 terms. In contrast, the underlying structure uses topic numbers based on the number of documents in each one.\n",
    "\n",
    "You can check the correspondence between the topic numbers in the data structure with those in the visualisation by listing them using the code below and matching up the list of terms.\n",
    "\n",
    "When listing items in a topic, remember to use the number from the model that corresponds to the topic in the visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa39f4f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0355ab67",
   "metadata": {},
   "source": [
    "Now you can list the documents by their main topic. Just set the topic number below and run the code. If the entry contains a digital object identifier (DOI), it is rendered as a URL that you can click. Some items do not have a DOI and you will need to search for them explicitly if you want to read them.\n",
    "\n",
    "If you want to save a set of items, uncomment the line at the end. This will create a csv file containing the items in the topic.\n",
    "\n",
    "For more display options with pandas dataframes, see the article on [Towards Data Science](https://towardsdatascience.com/6-tips-to-customize-the-display-of-your-pandas-data-frame-ce5a8caa7783)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5aeb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "topic_no = 10\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "wos_df = wos_df.assign(short = lambda x: x['TI'].str.slice(stop=120))\n",
    "wos_df = wos_df.assign(url = lambda x: 'https://dx.doi.org/' + x['DI'])\n",
    "HTML(wos_df.loc[wos_df['topic'] == topic_no, ['short', 'url']].to_html(render_links=True))\n",
    "\n",
    "# wos_df.loc[wos_df['topic'] == topic_no].to_csv('topic' + str(topic_no) + '.csv',  sep='\\t', index=False, index_label=wos_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18142de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
